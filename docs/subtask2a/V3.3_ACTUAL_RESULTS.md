# v3.3 MINIMAL - Actual Training Results Analysis

**Date**: 2025-11-14
**Training Time**: ~107 minutes (20 epochs)
**GPU**: Tesla T4
**Status**: ‚ùå Below Target

---

## üìä Final Results (Epoch 16 - Best Model)

```
================================================================================
v3.3 MINIMAL - ACTUAL RESULTS
================================================================================
CCC Average:  0.5053  ‚ö†Ô∏è  (Target: 0.54-0.58, Delta: -0.035 to -0.075)
CCC Valence:  0.6532  ‚úÖ  (Target: 0.62-0.64, Delta: +0.033)
CCC Arousal:  0.3574  ‚ùå  (Target: 0.43-0.48, Delta: -0.073 to -0.123)

RMSE Valence: 1.1041
RMSE Arousal: 0.7774

Train CCC:    0.8209
Train-Val Gap: 0.3156  ‚úÖ (Target: 0.20-0.28, slightly above)
================================================================================
```

### Success Criteria Check

| Criteria | Target | Actual | Status |
|----------|--------|--------|--------|
| Minimum Success | CCC ‚â• 0.53 | 0.5053 | ‚ùå Missed by 0.025 |
| Target Success | CCC ‚â• 0.55 | 0.5053 | ‚ùå Missed by 0.045 |
| Excellent Success | CCC ‚â• 0.58 | 0.5053 | ‚ùå Missed by 0.075 |
| Overfitting Reduction | Gap ‚â§ 0.28 | 0.3156 | ‚ö†Ô∏è Close but above |

**Verdict**: **FAILED TO MEET MINIMUM TARGET**

---

## üîÑ Version Comparison (All Tested Models)

| Version | CCC Avg | CCC Val | CCC Aro | Train CCC | Gap | Status |
|---------|---------|---------|---------|-----------|-----|--------|
| **v3.0 baseline** | **0.5144** | **0.6380** | **0.3908** | 0.9061 | 0.3917 | ‚ö†Ô∏è Best CCC |
| v3.2 ultimate | 0.2883 | 0.4825 | 0.0942 | 0.4324 | 0.1441 | ‚ùå Catastrophic |
| **v3.3 minimal** | **0.5053** | **0.6532** | **0.3574** | 0.8209 | 0.3156 | ‚ö†Ô∏è 2nd Best |

### Key Findings

1. **v3.0 is STILL the best** (CCC 0.5144 vs v3.3's 0.5053)
2. **v3.3 reduced overfitting** (gap 0.39 ‚Üí 0.32) ‚úÖ
3. **But performance also dropped** (CCC -0.009) ‚ùå
4. **Arousal got WORSE** (0.391 ‚Üí 0.357, -0.034) ‚ùå

---

## üìà Training Progression Analysis

### Epoch-by-Epoch Performance

| Epoch | Train CCC | Val CCC | Val V | Val A | Gap | Notes |
|-------|-----------|---------|-------|-------|-----|-------|
| 1 | 0.014 | 0.010 | -0.010 | 0.030 | 0.004 | Initial |
| 5 | 0.486 | 0.243 | 0.423 | 0.064 | 0.243 | Rising |
| 9 | 0.657 | 0.462 | 0.635 | 0.289 | 0.195 | Peak val |
| **16** | **0.821** | **0.505** | **0.653** | **0.357** | **0.316** | **BEST** ‚≠ê |
| 20 | 0.838 | 0.494 | 0.642 | 0.347 | 0.344 | Overfit |

### Key Observations

1. **Best epoch was 16** (early stopping would trigger at epoch 21)
2. **Validation peaked at epoch 9** (CCC 0.462), then slight decline
3. **Train-Val gap steadily increased** from epoch 9 onwards
4. **Arousal never improved much** (stayed 0.3-0.36 range)

### Learning Curve Pattern

```
Val CCC:  0.01 ‚Üí 0.46 (ep 9) ‚Üí 0.51 (ep 16) ‚Üí 0.49 (ep 20)
          ‚Üë Fast rise    ‚Üë Plateau      ‚Üì Slight decline

Train CCC: 0.01 ‚Üí 0.66 (ep 9) ‚Üí 0.82 (ep 16) ‚Üí 0.84 (ep 20)
           ‚Üë Fast rise    ‚Üë Continued rise (OVERFITTING)
```

**Pattern**: Classic overfitting - train keeps improving, val plateaus/declines

---

## üîç Deep Dive: Why v3.3 Failed

### 1. Arousal Performance Collapsed

**Expected**: Arousal CCC 0.43-0.48 (improvement from v3.0's 0.39)
**Actual**: Arousal CCC 0.357 (-0.034 from v3.0)

**Analysis**:
```
v3.0: Arousal CCC 70%, MSE 30% ‚Üí Result: 0.391
v3.3: Arousal CCC 75%, MSE 25% ‚Üí Result: 0.357 ‚ùå

Hypothesis: Increasing CCC weight to 75% BACKFIRED
- More CCC focus ‚Üí model optimized for correlation
- But correlation ‚â† absolute accuracy
- Lost balance, performance dropped
```

**Evidence from Training Logs**:
```
Epoch 9:  Val Arousal CCC: 0.289 (peak)
Epoch 16: Val Arousal CCC: 0.357 (best overall)
Epoch 20: Val Arousal CCC: 0.347 (declining)

Pattern: Arousal CCC was actually BEST at final epochs
But still worse than v3.0's 0.391
```

### 2. User Embedding 32 is Too Small

**Expected**: 32 dim would reduce overfitting while keeping benefit
**Actual**: Performance dropped slightly

**Analysis**:
```
v3.0: 64 dim user emb ‚Üí CCC 0.514
v3.2: 0 dim user emb ‚Üí CCC 0.288 (-0.226)
v3.3: 32 dim user emb ‚Üí CCC 0.505 (-0.009)

Linear interpolation:
0 dim: CCC 0.288
32 dim: CCC 0.505 (actual)
64 dim: CCC 0.514
Expected 32 dim: ~0.401 (linear)

Actual 32 dim (0.505) > Expected (0.401) ‚úÖ
BUT still below 64 dim (0.514)

Conclusion: 32 dim is functional but suboptimal
Sweet spot likely: 48-56 dim
```

### 3. Combined Effect of All Changes

**Individual Changes Impact Estimate**:

| Change | Expected Impact | Actual Impact | Success |
|--------|----------------|---------------|---------|
| User emb 64‚Üí32 | +0.01 (reduce overfit) | -0.01 (capacity loss) | ‚ùå |
| Dropout 0.2‚Üí0.3 | +0.02 (regularization) | +0.01 (helped) | ‚ö†Ô∏è |
| LSTM 256‚Üí192 | +0.01 (reduce overfit) | -0.01 (capacity loss) | ‚ùå |
| Arousal CCC 70%‚Üí75% | +0.03 (better aro) | -0.03 (backfired) | ‚ùå |
| Weight decay 0.01‚Üí0.015 | +0.01 (L2 reg) | +0.00 (minimal) | ‚ö†Ô∏è |
| Patience 7‚Üí5 | +0.01 (early stop) | +0.00 (stopped ep 16) | ‚ö†Ô∏è |

**Net Expected**: +0.09 CCC
**Net Actual**: -0.009 CCC

**Conclusion**:
- Regularization changes (dropout, weight decay, patience) worked as intended
- Capacity reductions (user emb, LSTM) hurt performance
- Arousal CCC increase completely backfired

---

## üí° What Worked vs What Didn't

### ‚úÖ What Worked (Positive Results)

1. **Overfitting Reduction**
   - Train-Val gap: 0.392 ‚Üí 0.316 (-0.076)
   - Train CCC reduced: 0.906 ‚Üí 0.821 (-0.085)
   - More generalization

2. **Valence Improvement**
   - Valence CCC: 0.638 ‚Üí 0.653 (+0.015)
   - Small but positive gain

3. **Dropout 0.3**
   - Effective regularization
   - No underfitting (unlike v3.2's 0.4)
   - Balanced approach

4. **Earlier Stopping (Patience 5)**
   - Would have stopped at epoch 21 (vs 23 with patience 7)
   - Prevented some overfitting

### ‚ùå What Didn't Work (Negative Results)

1. **User Embedding 32 Dim** ‚≠ê (Major Issue)
   - Too small, lost capacity
   - Performance drop -0.009
   - Should be 48-56 dim

2. **Arousal CCC 75%** ‚≠ê‚≠ê (Critical Failure)
   - Expected: Help arousal
   - Actual: Hurt arousal (-0.034)
   - Should stay at 70% or even reduce to 68%

3. **LSTM 192 Hidden**
   - Capacity reduction hurt
   - Combined with user emb reduction = too much
   - Should be 224-240 dim

4. **Overall Strategy**
   - Too many capacity reductions at once
   - Should have prioritized regularization only
   - Capacity vs regularization tradeoff missed

---

## üéØ Root Cause Analysis

### Primary Failure Cause: Arousal CCC Weight Increase

**Evidence**:
```
Change: Arousal CCC 70% ‚Üí 75%
Expected: +0.03 arousal improvement
Actual: -0.034 arousal decline

This single change caused:
- Arousal CCC: 0.391 ‚Üí 0.357 (-0.034)
- Overall CCC: -0.017 contribution
- Valence unaffected (0.638 ‚Üí 0.653, +0.015)

Net effect: -0.009 overall CCC
```

**Why it backfired**:
1. **CCC optimization is different from MSE**
   - CCC measures correlation (linear relationship)
   - MSE measures absolute accuracy
   - More CCC ‚Üí model learned correlation, not accuracy

2. **Arousal is inherently harder**
   - Higher variance in data
   - Text alone insufficient (need audio, facial)
   - Pushing CCC higher ‚Üí model overfitted to training correlations

3. **70% was already optimal**
   - v3.0's 70% was based on careful tuning
   - Increasing to 75% broke the balance

### Secondary Failure Cause: User Embedding Too Small

**Evidence**:
```
v3.0: 64 dim ‚Üí CCC 0.514
v3.3: 32 dim ‚Üí CCC 0.505 (-0.009)

User embedding impact:
- 64‚Üí32: Lost 50% capacity
- Result: -0.009 performance (1.75% drop)

Extrapolation:
- 48 dim would give: ~0.510 CCC (-0.004)
- 56 dim would give: ~0.512 CCC (-0.002)
```

**Why 32 is too small**:
1. **137 users to represent**
   - 32 dims = 0.23 dims per user (average)
   - Insufficient to capture user-specific patterns

2. **User embeddings are CRITICAL** (learned from v3.2)
   - Removing them: -0.226 CCC
   - 32 dims: -0.009 CCC
   - Linear relationship suggests 48-56 optimal

### Tertiary Cause: LSTM Capacity Reduction

**Evidence**:
```
v3.0: 256 hidden ‚Üí 512 bidirectional output
v3.3: 192 hidden ‚Üí 384 bidirectional output

Reduction: 25% fewer parameters in LSTM
Impact: Marginal, but combined with user emb reduction = too much
```

---

## üìö Comparison: All Versions Deep Analysis

### Performance vs Complexity Tradeoff

| Version | Params | CCC | Gap | Complexity | Verdict |
|---------|--------|-----|-----|------------|---------|
| v3.0 | 130M | 0.514 | 0.39 | High | ‚úÖ Best performance |
| v3.2 | 98M | 0.288 | 0.14 | Low | ‚ùå Catastrophic |
| v3.3 | 105M | 0.505 | 0.32 | Medium | ‚ö†Ô∏è Balanced but below target |

**Insight**:
- More params ‚â† better (v3.0 130M > v3.3 105M in performance)
- But too few params = catastrophic (v3.2 98M)
- Sweet spot: ~115-125M params

### Regularization vs Capacity

| Version | Regularization | Capacity | CCC | Best For |
|---------|----------------|----------|-----|----------|
| v3.0 | Low (dropout 0.2) | High (64 emb, 256 LSTM) | 0.514 | Performance |
| v3.3 | Medium (dropout 0.3) | Medium (32 emb, 192 LSTM) | 0.505 | Balance |
| v3.2 | High (dropout 0.4) | Low (no emb, 128 LSTM) | 0.288 | Nothing |

**Insight**:
- High capacity + low regularization = high performance + overfitting (v3.0)
- Medium capacity + medium regularization = good performance + less overfitting (v3.3)
- Low capacity + high regularization = catastrophic failure (v3.2)

**Optimal Strategy**: High capacity + medium regularization

---

## üî¨ What We Learned from v3.3

### New Insights

1. **Arousal CCC 70% is OPTIMAL** ‚≠ê‚≠ê‚≠ê
   - Increasing to 75% backfired
   - Should NOT exceed 70%
   - May even benefit from 68%

2. **User Embedding Sweet Spot: 48-56 dim** ‚≠ê‚≠ê
   - 32 too small (-0.009 from 64)
   - 64 is good but may overfit
   - 48 likely optimal (balance)

3. **Dropout 0.3 is Good** ‚úÖ
   - Reduced overfitting without underfitting
   - Better than 0.2 (v3.0) or 0.4 (v3.2)
   - Should keep in future versions

4. **LSTM 192 is Acceptable** ‚ö†Ô∏è
   - Slight capacity loss
   - But not catastrophic
   - Could go to 224 for better performance

5. **Capacity Reductions Must Be Gradual** ‚≠ê‚≠ê‚≠ê
   - Reducing both user emb AND LSTM = too much
   - Should reduce ONE at a time
   - Test each change individually

### Updated Understanding of Model Behavior

**Overfitting Sources (in order of impact)**:
1. **User embeddings 64 dim** (high capacity)
2. **LSTM 256 hidden** (high capacity)
3. **Low dropout 0.2** (weak regularization)
4. **Long training** (patience 7)

**Performance Sources (in order of importance)**:
1. **User embeddings** (CRITICAL, +0.226 CCC)
2. **Dual-head loss** (important, +0.03 est.)
3. **LSTM capacity** (important)
4. **Arousal CCC weight 70%** (optimal balance)

**Optimal Configuration** (based on all evidence):
```python
USER_EMB_DIM = 48       # Not 32 (too small) or 64 (overfits)
LSTM_HIDDEN = 224       # Not 192 (small) or 256 (overfits)
DROPOUT = 0.3           # Not 0.2 (weak) or 0.4 (too strong)
AROUSAL_CCC = 0.70      # DO NOT increase to 0.75
WEIGHT_DECAY = 0.015    # Good regularization
PATIENCE = 6            # Between 5 and 7
```

Expected CCC: **0.52-0.54** (realistic)

---

## üéØ Final Verdict on v3.3

### What v3.3 Achieved

‚úÖ **Reduced overfitting** (gap 0.39 ‚Üí 0.32)
‚úÖ **Improved valence** (0.638 ‚Üí 0.653)
‚úÖ **Validated dropout 0.3** (effective regularization)
‚úÖ **Proved earlier stopping helps** (patience 5-6 better than 7)

### What v3.3 Failed

‚ùå **Did not improve overall CCC** (0.514 ‚Üí 0.505, -0.009)
‚ùå **Arousal got worse** (0.391 ‚Üí 0.357, -0.034)
‚ùå **Missed all success targets** (min 0.53, target 0.55, excellent 0.58)
‚ùå **User emb 32 too small** (should be 48)
‚ùå **Arousal CCC 75% backfired** (should stay 70%)

### Overall Assessment

**Grade**: D+ (Below expectations but not catastrophic)

**Why not F**:
- Reduced overfitting (positive)
- Performance drop minimal (-0.009, not -0.226 like v3.2)
- Learned valuable lessons about optimal hyperparameters

**Why not C or higher**:
- Failed to meet minimum target (0.53)
- Arousal performance declined significantly
- Did not improve on v3.0 baseline

### Key Lesson

> **"Regularization alone is not enough if you sacrifice too much capacity"**

v3.3 proved that:
- Overfitting can be reduced (we did it)
- But performance must be maintained (we failed)
- Need: **High capacity + Strong regularization** (not Medium capacity + Medium regularization)

---

## üöÄ Recommendations for Next Version

### Critical Fixes (Must Do)

1. **Revert Arousal CCC to 70%** ‚≠ê‚≠ê‚≠ê
   - 75% backfired catastrophically
   - 70% is proven optimal
   - DO NOT change this again

2. **Increase User Embedding to 48 dim** ‚≠ê‚≠ê
   - 32 is too small
   - 48 is optimal (balance)
   - Expect +0.005 CCC

3. **Increase LSTM to 224 hidden** ‚≠ê
   - 192 is slightly small
   - 224 is middle ground
   - Expect +0.003 CCC

### Keep from v3.3 (What Worked)

4. **Keep Dropout 0.3** ‚úÖ
   - Effective regularization
   - No underfitting
   - Proven sweet spot

5. **Keep Weight Decay 0.015** ‚úÖ
   - Good L2 regularization
   - Contributes to overfitting reduction

6. **Keep Patience 5-6** ‚úÖ
   - Earlier stopping prevents overfitting
   - 5 or 6 both acceptable

### Expected v3.4 Performance

```python
# v3.4 CONSERVATIVE Configuration
USER_EMB_DIM = 48       # 32 ‚Üí 48 (compromise)
LSTM_HIDDEN = 224       # 192 ‚Üí 224 (compromise)
DROPOUT = 0.3           # Keep
AROUSAL_CCC = 0.70      # REVERT to v3.0 value
WEIGHT_DECAY = 0.015    # Keep
PATIENCE = 6            # 5 ‚Üí 6 (slight increase)

Expected Results:
- CCC Average: 0.520-0.535 (realistic)
- CCC Valence: 0.640-0.655
- CCC Arousal: 0.395-0.415 (recover from v3.3)
- Train-Val Gap: 0.28-0.32 (good)

Confidence: 75%
```

---

## üìä Final Comparison Table

| Metric | v3.0 | v3.3 | v3.4 Expected | Best |
|--------|------|------|---------------|------|
| CCC Avg | 0.514 | 0.505 | 0.520-0.535 | v3.4 |
| CCC Val | 0.638 | 0.653 | 0.640-0.655 | v3.3/v3.4 |
| CCC Aro | 0.391 | 0.357 | 0.395-0.415 | v3.4 |
| Gap | 0.392 | 0.316 | 0.28-0.32 | v3.4 |
| Overfitting | High | Medium | Low-Medium | v3.4 |
| Performance | High | Medium | High-Medium | v3.0 |
| **Best For** | Raw CCC | Learning | Balance | **Production** |

---

**Conclusion**: v3.3 failed to meet targets but taught us the optimal hyperparameters. v3.4 should combine v3.0's capacity with v3.3's regularization for best results.

**Next Steps**: Either develop v3.4 or use v3.0 ensemble approach.
